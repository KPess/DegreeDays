{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778d8022-686e-4c94-97f5-edba18938dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1054e409-1f25-48b0-aab0-5ade1936f2d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2bcd91-a1ca-42ec-937c-896df0c9dead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import temperature and population files\n",
    "df = pd.read_csv('../SynMax/Population Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "069cc539-c615-4a67-8348-444819ffac30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import temperature and population files\n",
    "df2 = pd.read_csv('../Synmax/Temperature Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create time series including all relevant dates\n",
    "time = pd.date_range(start = '2015-01-01', end = '2021-04-20', freq='D' )\n",
    "s = pd.Series(index=time)\n",
    "\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import temperature and population files\n",
    "# Sanitize temp data\n",
    "    # Convert location_date to datetime\n",
    "    # Set datettime as index\n",
    "        # Iterate through stations one by one to find station's missing days\n",
    "            # Find missing days\n",
    "            # Iterate through missing dates\n",
    "                # Add date row\n",
    "                # Add other relevant info - station_code, region, etc\n",
    "                # Add boolean column indicating interpolated row\n",
    "                # Interpolate temperatures linearly from previous and next day from same station\n",
    "                # Weigh city temp entries by population versus entire region\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5cd52ee-e2d5-421a-a3a9-8367e3023f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset grouped by date and name\n",
    "df2bydate = df2.groupby(['location_date', 'name'])[['temp_mean_c', 'temp_min_c', 'temp_max_c']].agg(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame rearranged by date \n",
    "datetime = pd.to_datetime(df2['location_date'])\n",
    "df2['date'] = datetime.dt.date\n",
    "df2 = df2.set_index(\"date\")\n",
    "\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for number of city names in temp data\n",
    "cnt2 = 0\n",
    "cities2 = []\n",
    "for i in range(0, len(df2)):\n",
    "    if df2['name'][i] not in cities2:\n",
    "        cities2.append(df2['name'][i])\n",
    "        cnt2 += 1\n",
    "\n",
    "print(\"No of cities:\", cnt2)\n",
    "print(\"Cities:\", cities2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for unique cities in pop data\n",
    "cnt1 = 0\n",
    "cities1 = []\n",
    "for i in range(0, len(df)):\n",
    "    if df['City'][i] not in cities1:\n",
    "        cities1.append(df['City'][i])\n",
    "        cnt1 += 1\n",
    "\n",
    "print(\"No of unique cities:\", cnt1)\n",
    "print(\"Cities:\", cities1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide the country into regions\n",
    "Pacific = ['Washington', 'Oregon', 'California', 'Hawaii', 'Alaska']\n",
    "Mountain = ['Montana', 'Idaho', 'Wyoming', 'Colorado', 'Utah', 'Nevada', 'Arizona', 'New Mexico']\n",
    "WestNorthCentral = ['North Dakota', 'South Dakota', 'Nebraska', 'Kansas', 'Minnesota', 'Iowa', 'Missouri']\n",
    "WestSouthCentral = ['Texas', 'Oklahoma', 'Arkansas', 'Louisiana']\n",
    "EastNorthCentral = ['Michigan', 'Wisconsin', 'Illinois', 'Indiana', 'Ohio']\n",
    "EastSouthCentral = ['Kentucky', 'Tennessee', 'Mississippi', 'Alabama']\n",
    "SouthAtlantic = ['Florida', 'Georgia', 'South Carolina', 'North Carolina', 'Virginia', 'West Virginia', 'Maryland', 'Delaware', 'District of Columbia']\n",
    "MidAtlantic = ['Pennsylvania', 'New York', 'New Jersey']\n",
    "NorthAtlantic = ['Maine', 'Vermont', 'New Hampshire', 'Massachusetts', 'Connecticut', 'Rhode Island']\n",
    "\n",
    "statecount = len(Pacific) + len(Mountain) + len(WestNorthCentral) + len(WestSouthCentral) + len(EastNorthCentral) + len(EastSouthCentral) + len(SouthAtlantic) + len(MidAtlantic) + len(NorthAtlantic)\n",
    "\n",
    "print(statecount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regions = [Pacific, Mountain, WestNorthCentral, WestSouthCentral, EastNorthCentral, EastSouthCentral, SouthAtlantic, MidAtlantic, NorthAtlantic]\n",
    "print(regions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for duplicate city names with different station code - 39 stations 38 cities # KPDX Portland Oregon and KPWM Portland Maine\n",
    "cnt3 = 0\n",
    "stations = []\n",
    "for i in range(0, len(df2)):\n",
    "    if df2['station_code'][i] not in stations:\n",
    "        stations.append(df2['station_code'][i])\n",
    "        cnt3 += 1\n",
    "\n",
    "print(\"No of unique stations:\", cnt3)\n",
    "print(\"Stations:\", stations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign states to stations\n",
    "df2.loc[df2['station_code']== 'KBDL', ['state']] = 'Connecticut'\n",
    "df2.loc[df2['station_code']== 'KATL', ['state']] = 'Georgia'\n",
    "df2.loc[df2['station_code']== 'KBNA', ['state']] = 'Tennessee'\n",
    "df2.loc[df2['station_code']== 'KBOI', ['state']] = 'Idaho'\n",
    "df2.loc[df2['station_code']== 'KBOS', ['state']] = 'Massachusetts'\n",
    "df2.loc[df2['station_code']== 'KBUF', ['state']] = 'New York'\n",
    "df2.loc[df2['station_code']== 'KBUR', ['state']] = 'California'\n",
    "df2.loc[df2['station_code']== 'KBWI', ['state']] = 'Maryland'\n",
    "df2.loc[df2['station_code']== 'KCMH', ['state']] = 'Ohio'\n",
    "df2.loc[df2['station_code']== 'KCQT', ['state']] = 'California'\n",
    "df2.loc[df2['station_code']== 'KCVG', ['state']] = 'Kentucky'\n",
    "df2.loc[df2['station_code']== 'KDCA', ['state']] = 'Virginia'\n",
    "df2.loc[df2['station_code']== 'KDEN', ['state']] = 'Colorado'\n",
    "df2.loc[df2['station_code']== 'KDFW', ['state']] = 'Texas'\n",
    "df2.loc[df2['station_code']== 'KDTW', ['state']] = 'Michigan'\n",
    "df2.loc[df2['station_code']== 'KFAT', ['state']] = 'California'\n",
    "df2.loc[df2['station_code']== 'KGEG', ['state']] = 'Washington'\n",
    "df2.loc[df2['station_code']== 'KIAD', ['state']] = 'Virginia'\n",
    "df2.loc[df2['station_code']== 'KIAH', ['state']] = 'Texas'\n",
    "df2.loc[df2['station_code']== 'KLAS', ['state']] = 'Nevada'\n",
    "df2.loc[df2['station_code']== 'KLGA', ['state']] = 'New York'\n",
    "df2.loc[df2['station_code']== 'KLIT', ['state']] = 'Arkansas'\n",
    "df2.loc[df2['station_code']== 'KMEM', ['state']] = 'Tennessee'\n",
    "df2.loc[df2['station_code']== 'KMSP', ['state']] = 'Minnesota'\n",
    "df2.loc[df2['station_code']== 'KMSY', ['state']] = 'Louisiana'\n",
    "df2.loc[df2['station_code']== 'KORD', ['state']] = 'Illinois'\n",
    "df2.loc[df2['station_code']== 'KPDX', ['state']] = 'Oregon'\n",
    "df2.loc[df2['station_code']== 'KPHL', ['state']] = 'Pennsylvania'\n",
    "df2.loc[df2['station_code']== 'KPHX', ['state']] = 'Arizona'\n",
    "df2.loc[df2['station_code']== 'KPIT', ['state']] = 'Pennsylvania'\n",
    "df2.loc[df2['station_code']== 'KPWM', ['state']] = 'Maine'\n",
    "df2.loc[df2['station_code']== 'KRDU', ['state']] = 'North Carolina'\n",
    "df2.loc[df2['station_code']== 'KRIC', ['state']] = 'Virginia'\n",
    "df2.loc[df2['station_code']== 'KSAC', ['state']] = 'California'\n",
    "df2.loc[df2['station_code']== 'KSEA', ['state']] = 'Washington'\n",
    "df2.loc[df2['station_code']== 'KSFO', ['state']] = 'California'\n",
    "df2.loc[df2['station_code']== 'KSLC', ['state']] = 'Utah'\n",
    "df2.loc[df2['station_code']== 'KSTL', ['state']] = 'Missouri'\n",
    "df2.loc[df2['station_code']== 'KALB', ['state']] = 'New York'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Assign states to regions in temperature dataset\n",
    "\n",
    "region = []\n",
    "for i in range(0, len(df2)):\n",
    "    for j in range(0, len(regions)):\n",
    "        if df2['state'][i] in regions[j]:\n",
    "            region.append(j)\n",
    "# Establish region column in temp table      \n",
    "df2[\"region\"] = region  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assign states to regions in population dataset\n",
    "\n",
    "region2 = []\n",
    "for k in range(0, len(df)):\n",
    "    for l in range(0, len(regions)):\n",
    "        if df['State'][k] in regions[l]:\n",
    "            region2.append(l)\n",
    "\n",
    "# Establish region column in pop table          \n",
    "df[\"region\"] = region2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes as .csv\n",
    "df2.to_csv(path_or_buf='../SynMax/Temp Data.csv')\n",
    "df.to_csv(path_or_buf='../SynMax/Pop Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert date to datetime and fill\n",
    "datetime = pd.to_datetime(df2['location_date'])\n",
    "df2['date'] = datetime.dt.date\n",
    "df2 = df2.set_index(\"date\")\n",
    "print(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sub-frame of each specific station\n",
    "KATL = df2.loc[df2['station_code'] == stations[0]]\n",
    "KBDL = df2.loc[df2['station_code'] == stations[1]]\n",
    "KBNA = df2.loc[df2['station_code'] == stations[2]]\n",
    "KBOI = df2.loc[df2['station_code'] == stations[3]]\n",
    "KBOS = df2.loc[df2['station_code'] == stations[4]]\n",
    "KBUF = df2.loc[df2['station_code'] == stations[5]]\n",
    "KBUR = df2.loc[df2['station_code'] == stations[6]]\n",
    "KBWI = df2.loc[df2['station_code'] == stations[7]]\n",
    "KCMH = df2.loc[df2['station_code'] == stations[8]]\n",
    "KCQT = df2.loc[df2['station_code'] == stations[9]]\n",
    "KCVG = df2.loc[df2['station_code'] == stations[10]]\n",
    "KDCA = df2.loc[df2['station_code'] == stations[11]]\n",
    "KDEN = df2.loc[df2['station_code'] == stations[12]]\n",
    "KDFW = df2.loc[df2['station_code'] == stations[13]]\n",
    "KDTW = df2.loc[df2['station_code'] == stations[14]]\n",
    "KFAT = df2.loc[df2['station_code'] == stations[15]]\n",
    "KGEG = df2.loc[df2['station_code'] == stations[16]]\n",
    "KIAD = df2.loc[df2['station_code'] == stations[17]]\n",
    "KIAH = df2.loc[df2['station_code'] == stations[18]]\n",
    "KLAS = df2.loc[df2['station_code'] == stations[19]]\n",
    "KLGA = df2.loc[df2['station_code'] == stations[20]]\n",
    "KLIT = df2.loc[df2['station_code'] == stations[21]]\n",
    "KMEM = df2.loc[df2['station_code'] == stations[22]]\n",
    "KMSP = df2.loc[df2['station_code'] == stations[23]]\n",
    "KMSY = df2.loc[df2['station_code'] == stations[24]]\n",
    "KORD = df2.loc[df2['station_code'] == stations[25]]\n",
    "KPDX = df2.loc[df2['station_code'] == stations[26]]\n",
    "KPHL = df2.loc[df2['station_code'] == stations[27]]\n",
    "KPHX = df2.loc[df2['station_code'] == stations[28]]\n",
    "KPIT = df2.loc[df2['station_code'] == stations[29]]\n",
    "KPWM = df2.loc[df2['station_code'] == stations[30]]\n",
    "KRDU = df2.loc[df2['station_code'] == stations[31]]\n",
    "KRIC = df2.loc[df2['station_code'] == stations[32]]\n",
    "KSAC = df2.loc[df2['station_code'] == stations[33]]\n",
    "KSEA = df2.loc[df2['station_code'] == stations[34]]\n",
    "KSFO = df2.loc[df2['station_code'] == stations[35]]\n",
    "KSLC = df2.loc[df2['station_code'] == stations[36]]\n",
    "KSTL = df2.loc[df2['station_code'] == stations[37]]\n",
    "KALB = df2.loc[df2['station_code'] == stations[38]]\n",
    "print(KALB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create loop to sanitize data from each station subset\n",
    "for i in range(0, len(stations)):\n",
    "    stations[1]['missing'] = 0\n",
    "print(stations[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column indicating if data was missing\n",
    "KATL['missing'] = '0'\n",
    "print(KATL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add missing dates to station sub-frame\n",
    "time = pd.date_range(start = '2015-01-01', end = '2021-04-20', freq='D' )\n",
    "sTime = pd.Series(index=time)\n",
    "KATL = pd.concat([KATL, sTime[~sTime.index.isin(KATL.index)]]).sort_index()\n",
    "KATL = KATL.drop([0],axis=1)\n",
    "\n",
    "print(KATL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flag missing dates as missing\n",
    "\n",
    "# Find rows where data is missing\n",
    "missingKATL = KATL.loc[KATL['missing'].isna() ]\n",
    "\n",
    "# Set values in missing subset \n",
    "# Set missing to 1\n",
    "missingKATL.loc[:,'missing'] = '1'\n",
    "\n",
    "# Set name to station name\n",
    "missingKATL.loc[:, 'name'] = 'Atlanta'\n",
    "\n",
    "# Set region to station region\n",
    "missingKATL.loc[:, 'region'] = '6'\n",
    "\n",
    "# Set state to station state\n",
    "missingKATL.loc[:, 'state'] = 'Georgia'\n",
    "\n",
    "# Set station_code to station\n",
    "missingKATL.loc[:, 'station_code'] = 'KATL'\n",
    "\n",
    "# Remove empty columns of temp data\n",
    "missingKATL.drop(columns=['temp_min_c', 'temp_mean_c', 'temp_max_c', 'location_date'], inplace=True, axis=0)\n",
    "\n",
    "print(missingKATL)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpolate temp_min_c within KATL set \n",
    "KATLinterpolMIN = KATL['temp_min_c'].interpolate()\n",
    "print(KATLinterpolMIN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpolate temp_max_c within KATL set \n",
    "KATLinterpolMAX = KATL['temp_max_c'].interpolate()\n",
    "print(KATLinterpolMAX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Interpolate temp_mean_c within KATL set \n",
    "KATLinterpolMEAN = KATL['temp_mean_c'].interpolate()\n",
    "print(KATLinterpolMEAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate sub-frames of interpolated temperature data into single frame\n",
    "KATLinterpol = pd.concat([KATLinterpolMAX, KATLinterpolMIN, KATLinterpolMEAN], axis=1)\n",
    "\n",
    "print(KATLinterpol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new empty frame and integrate missing date row frames with interpolated temp frame\n",
    "KATL2 = []\n",
    "KATL2 = pd.DataFrame(s)\n",
    "KATL2 = pd.concat([KATLinterpol, missingKATL], axis=1)\n",
    "KATL2 = KATL2.dropna()\n",
    "print(KATL2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop all NaN rows from original station subframe and combine cleaned and interpolated subframes\n",
    "KATL = KATL.dropna()\n",
    "KATL3 = pd.concat([KATL, KATL2], axis=0)\n",
    "# Review complete sub-frame of KATL \n",
    "print(KATL3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add column indicating if data was missing\n",
    "KSTL['missing'] = '0'\n",
    "\n",
    "# Add missing dates to station sub-frame\n",
    "time = pd.date_range(start = '2015-01-01', end = '2021-04-20', freq='D' )\n",
    "sTime = pd.Series(index=time)\n",
    "KSTL = pd.concat([KSTL, sTime[~sTime.index.isin(KSTL.index)]]).sort_index()\n",
    "KSTL = KSTL.drop([0],axis=1)\n",
    "\n",
    "# Find rows where data is missing\n",
    "missingKSTL = KSTL.loc[KSTL['missing'].isna() ]\n",
    "# Set values in missing subset \n",
    "# Set missing to 1\n",
    "missingKSTL.loc[:,'missing'] = '1'\n",
    "\n",
    "# Set name to station name\n",
    "missingKSTL.loc[:, 'name'] = 'St Louis/Lambert'\n",
    "\n",
    "# Set region to station region\n",
    "missingKSTL.loc[:, 'region'] = '2'\n",
    "\n",
    "# Set state to station state\n",
    "missingKSTL.loc[:, 'state'] = 'Missouri'\n",
    "\n",
    "# Set station_code to station\n",
    "missingKSTL.loc[:, 'station_code'] = 'KSTL'\n",
    "\n",
    "# Remove empty columns of temp data\n",
    "missingKSTL.drop(columns=['temp_min_c', 'temp_mean_c', 'temp_max_c', 'location_date'], inplace=True, axis=0)\n",
    "\n",
    "#Interpolate temp_min_c within KSTL set \n",
    "KSTLinterpolMIN = KSTL['temp_min_c'].interpolate()\n",
    "\n",
    "#Interpolate temp_max_c within KSTL set \n",
    "KSTLinterpolMAX = KSTL['temp_max_c'].interpolate()\n",
    "\n",
    "#Interpolate temp_mean_c within KSTL set \n",
    "KSTLinterpolMEAN = KSTL['temp_mean_c'].interpolate()\n",
    "\n",
    "# Concatenate sub-frames of interpolated temperature data into single frame\n",
    "KSTLinterpol = pd.concat([KSTLinterpolMAX, KSTLinterpolMIN, KSTLinterpolMEAN], axis=1)\n",
    "\n",
    "# Create new empty frame and integrate missing date row frames with interpolated temp frame\n",
    "KSTL2 = []\n",
    "KSTL2 = pd.DataFrame(s)\n",
    "KSTL2 = pd.concat([KSTLinterpol, missingKSTL], axis=1)\n",
    "KSTL2 = KSTL2.dropna()\n",
    "\n",
    "# Drop all NaN rows from original station subframe and combine cleaned and interpolated subframes\n",
    "KSTL = KSTL.dropna()\n",
    "KSTL3 = pd.concat([KSTL, KSTL2], axis=0)\n",
    "# Review complete sub-frame of KSTL \n",
    "print(KSTL3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Append all wrangled station dataframes into one\n",
    "df3 = pd.DataFrame(KATL3)\n",
    "\n",
    "df3 = pd.concat([df3, KBDL3, KBNA3, KBOI3, KBOS3, KBUF3, KBUR3, KBWI3, KCMH3, KCQT3], axis=0)\n",
    "df3 = pd.concat([df3, KDCA3, KDEN3, KDFW3, KDTW3, KFAT3, KGEG3, KIAD3, KIAH3, KLAS3], axis=0)\n",
    "df3 = pd.concat([df3, KLIT3, KMEM3, KMSP3, KMSY3, KORD3, KPDX3, KPHL3, KPHX3, KPIT3], axis=0)\n",
    "df3 = pd.concat([df3, KRDU3, KRIC3, KSAC3, KSEA3, KSFO3, KSLC3, KSTL3, KALB3, KCVG3], axis=0)\n",
    "df3 = pd.concat([df3, KPWM3, KLGA3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(path_or_buf='../SynMax/Temp Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = pd.concat([KBDL3, KBNA3, KBOI3, KBOS3, KBUF3, KBUR3, KBWI3, KCMH3, KCQT3, KCVG3], axis=0)\n",
    "df3 = pd.concat([KDCA3, KDEN3, KDFW3, KDTW3, KFAT3, KGEG3, KIAD3, KIAH3, KLAS3, KLGA3], axis=0)\n",
    "df3 = pd.concat([KLIT3, KMEM3, KMSP3, KMSY3, KORD3, KPDX3, KPHL3, KPHX3, KPIT3, KPWM3], axis=0)\n",
    "df3 = pd.concat([KRDU3, KRIC3, KSAC3, KSEA3, KSFO3, KSLC3, KSTL3, KALB3], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                City          State  population       Lon      Lat  region\n0          Henderson         Nevada      260068 -115.0375  36.0122       1\n1         Manchester  New Hampshire      109830  -71.4439  42.9847       8\n2          Elizabeth     New Jersey      125660  -74.1935  40.6663       7\n3             Newark     New Jersey      277540  -74.1726  40.7242       7\n4           Paterson     New Jersey      146427  -74.1628  40.9147       7\n..               ...            ...         ...       ...      ...     ...\n280            Miami        Florida      408750  -80.2086  25.7752       6\n281        Hollywood        Florida      143357  -80.1646  26.0311       6\n282  Fort Lauderdale        Florida      168528  -80.1439  26.1413       6\n283    Pompano Beach        Florida      101617  -80.1290  26.2426       6\n284  West Palm Beach        Florida      101043  -80.1266  26.7483       6\n\n[285 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1 Someone who wants to know how the temperature compares to the seasonal average as well as high and low cases. \n",
    "\n",
    "# 2 Someone who wants to view the monthly average, min and max. \n",
    "\n",
    "# 3 Someone who wants to graphically see what data is missing or projected. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here are 4 ways to select all rows with NaN values in Pandas DataFrame:\n",
    "\n",
    "# (1) Using isna() to select all rows with NaN under a single DataFrame column:\n",
    "# df[df['column name'].isna()]\n",
    "\n",
    "# (2) Using isnull() to select all rows with NaN under a single DataFrame column:\n",
    "# df[df['column name'].isnull()]\n",
    "\n",
    "# (3) Using isna() to select all rows with NaN under an entire DataFrame:\n",
    "# df[df.isna().any(axis=1)]\n",
    "\n",
    "# (4) Using isnull() to select all rows with NaN under an entire DataFrame:\n",
    "# df[df.isnull().any(axis=1)]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "interpreter": {
   "hash": "e96f6acdfbe39255c7875dee1b1acc7feafbd646fa220a924b021b041aac88e9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}