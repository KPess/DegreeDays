{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1125,
   "id": "778d8022-686e-4c94-97f5-edba18938dc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1126,
   "id": "1054e409-1f25-48b0-aab0-5ade1936f2d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1127,
   "id": "0c2bcd91-a1ca-42ec-937c-896df0c9dead",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../SynMax/Population Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1128,
   "id": "069cc539-c615-4a67-8348-444819ffac30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2 = pd.read_csv('../Synmax/Temperature Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1129,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfUScities = pd.read_csv('../Synmax/uscities.csv')\n",
    "\n",
    "#import from https://simplemaps.com/data/us-cities to check both dataframes against and sort into regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1130,
   "id": "1b62fea8-b2bc-4c3b-8c63-e8f0c1c84cab",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "            name station_code location_date  temp_mean_c  temp_min_c  \\\n",
       "0        Atlanta         KATL     4/20/2021    17.491667        12.2   \n",
       "1  Windsor Locks         KBDL     4/20/2021    13.887500         3.9   \n",
       "2      Nashville         KBNA     4/20/2021    15.175000         6.7   \n",
       "3          Boise         KBOI     4/20/2021     9.329167         1.7   \n",
       "4         Boston         KBOS     4/20/2021    16.716667        10.0   \n",
       "\n",
       "   temp_max_c  state  region  \n",
       "0        23.3    NaN     NaN  \n",
       "1        22.2    NaN     NaN  \n",
       "2        23.3    NaN     NaN  \n",
       "3        15.6    NaN     NaN  \n",
       "4        23.9    NaN     NaN  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>station_code</th>\n      <th>location_date</th>\n      <th>temp_mean_c</th>\n      <th>temp_min_c</th>\n      <th>temp_max_c</th>\n      <th>state</th>\n      <th>region</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Atlanta</td>\n      <td>KATL</td>\n      <td>4/20/2021</td>\n      <td>17.491667</td>\n      <td>12.2</td>\n      <td>23.3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Windsor Locks</td>\n      <td>KBDL</td>\n      <td>4/20/2021</td>\n      <td>13.887500</td>\n      <td>3.9</td>\n      <td>22.2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Nashville</td>\n      <td>KBNA</td>\n      <td>4/20/2021</td>\n      <td>15.175000</td>\n      <td>6.7</td>\n      <td>23.3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Boise</td>\n      <td>KBOI</td>\n      <td>4/20/2021</td>\n      <td>9.329167</td>\n      <td>1.7</td>\n      <td>15.6</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Boston</td>\n      <td>KBOS</td>\n      <td>4/20/2021</td>\n      <td>16.716667</td>\n      <td>10.0</td>\n      <td>23.9</td>\n      <td>NaN</td>\n      <td>NaN</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 1130
    }
   ],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a175c2-2c2b-4a55-b9e9-847458758669",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1131,
   "id": "01c33702-f375-4571-8184-1235f3a88a7b",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['City', 'State', 'population', 'Lon', 'Lat', 'region'], dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 1131
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1132,
   "id": "032be9d7-b6df-47e8-9c92-6da70258f880",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Index(['name', 'station_code', 'location_date', 'temp_mean_c', 'temp_min_c',\n",
       "       'temp_max_c', 'state', 'region'],\n",
       "      dtype='object')"
      ]
     },
     "metadata": {},
     "execution_count": 1132
    }
   ],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1133,
   "id": "160827fa-97b2-43c4-a8cc-63739717ef13",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([['Henderson', 'Nevada', 260068, -115.0375, 36.0122, nan],\n",
       "       ['Manchester', 'New Hampshire', 109830, -71.4439, 42.9847, nan],\n",
       "       ['Elizabeth', 'New Jersey', 125660, -74.1935, 40.6663, nan],\n",
       "       ...,\n",
       "       ['Fort Lauderdale', 'Florida', 168528, -80.1439, 26.1413, nan],\n",
       "       ['Pompano Beach', 'Florida', 101617, -80.129, 26.2426, nan],\n",
       "       ['West Palm Beach', 'Florida', 101043, -80.1266, 26.7483, nan]],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 1133
    }
   ],
   "source": [
    "df.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1134,
   "id": "1bf8d4ba-a69f-4dfb-8d70-ac10e7dcb812",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([['Atlanta', 'KATL', '4/20/2021', ..., 23.3, nan, nan],\n",
       "       ['Windsor Locks', 'KBDL', '4/20/2021', ..., 22.2, nan, nan],\n",
       "       ['Nashville', 'KBNA', '4/20/2021', ..., 23.3, nan, nan],\n",
       "       ...,\n",
       "       ['Raleigh/Durham', 'KRDU', '1/1/2015', ..., 10.6, nan, nan],\n",
       "       ['Pittsburgh', 'KPIT', '1/1/2015', ..., 1.7, nan, nan],\n",
       "       ['Phoenix/Sky HRBR', 'KPHX', '1/1/2015', ..., 7.2, nan, nan]],\n",
       "      dtype=object)"
      ]
     },
     "metadata": {},
     "execution_count": 1134
    }
   ],
   "source": [
    "df2.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1135,
   "id": "c5cd52ee-e2d5-421a-a3a9-8367e3023f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2bydate = df2.groupby(['location_date', 'name'])[['temp_mean_c', 'temp_min_c', 'temp_max_c']].agg(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1136,
   "id": "ff870ee8-861b-4023-9ee8-2590a7528d1a",
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                                temp_mean_c  temp_min_c  temp_max_c\n",
       "location_date name                                                 \n",
       "1/1/2015      Albany              -3.362500        -7.2         0.0\n",
       "              Atlanta              6.758333         0.0        13.9\n",
       "              Baltimore           -0.345833        -7.8         6.1\n",
       "              Boise              -11.070833       -13.9        -8.0\n",
       "              Boston              -2.329167        -5.6         0.6\n",
       "...                                     ...         ...         ...\n",
       "9/9/2020      Spokane             17.358333         8.9        26.1\n",
       "              St Louis/Lambert    23.937500        19.4        29.4\n",
       "              Wash DC/Dulles      23.179167        20.0        26.1\n",
       "              Washington          24.229167        22.8        26.1\n",
       "              Windsor Locks       22.425000        18.3        27.8\n",
       "\n",
       "[86932 rows x 3 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th></th>\n      <th>temp_mean_c</th>\n      <th>temp_min_c</th>\n      <th>temp_max_c</th>\n    </tr>\n    <tr>\n      <th>location_date</th>\n      <th>name</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">1/1/2015</th>\n      <th>Albany</th>\n      <td>-3.362500</td>\n      <td>-7.2</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>Atlanta</th>\n      <td>6.758333</td>\n      <td>0.0</td>\n      <td>13.9</td>\n    </tr>\n    <tr>\n      <th>Baltimore</th>\n      <td>-0.345833</td>\n      <td>-7.8</td>\n      <td>6.1</td>\n    </tr>\n    <tr>\n      <th>Boise</th>\n      <td>-11.070833</td>\n      <td>-13.9</td>\n      <td>-8.0</td>\n    </tr>\n    <tr>\n      <th>Boston</th>\n      <td>-2.329167</td>\n      <td>-5.6</td>\n      <td>0.6</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th rowspan=\"5\" valign=\"top\">9/9/2020</th>\n      <th>Spokane</th>\n      <td>17.358333</td>\n      <td>8.9</td>\n      <td>26.1</td>\n    </tr>\n    <tr>\n      <th>St Louis/Lambert</th>\n      <td>23.937500</td>\n      <td>19.4</td>\n      <td>29.4</td>\n    </tr>\n    <tr>\n      <th>Wash DC/Dulles</th>\n      <td>23.179167</td>\n      <td>20.0</td>\n      <td>26.1</td>\n    </tr>\n    <tr>\n      <th>Washington</th>\n      <td>24.229167</td>\n      <td>22.8</td>\n      <td>26.1</td>\n    </tr>\n    <tr>\n      <th>Windsor Locks</th>\n      <td>22.425000</td>\n      <td>18.3</td>\n      <td>27.8</td>\n    </tr>\n  </tbody>\n</table>\n<p>86932 rows × 3 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 1136
    }
   ],
   "source": [
    "df2bydate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1137,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No of unique cities: 38\nCities: ['Atlanta', 'Windsor Locks', 'Nashville', 'Boise', 'Boston', 'Buffalo', 'Burbank', 'Baltimore', 'Columbus', 'Los Angeles', 'Covington', 'Washington', 'Denver', 'Dallas', 'Detroit/Wayne', 'Fresno', 'Spokane', 'Wash DC/Dulles', 'Houston', 'Las Vegas', 'NYC/LaGuardia', 'Little Rock', 'Memphis', 'Minneapolis', 'New Orleans', \"Chicago O'Hare\", 'Portland', 'Philadelphia', 'Phoenix/Sky HRBR', 'Pittsburgh', 'Raleigh/Durham', 'Richmond', 'Sacramento/Execu', 'Seattle', 'San Francisco', 'Salt Lake City', 'St Louis/Lambert', 'Albany']\n"
     ]
    }
   ],
   "source": [
    "# Check for unique cities in temp data\n",
    "cnt2 = 0\n",
    "cities2 = []\n",
    "for i in range(0, len(df2)):\n",
    "    if df2['name'][i] not in cities2:\n",
    "        cities2.append(df2['name'][i])\n",
    "        cnt2 += 1\n",
    "\n",
    "print(\"No of unique cities:\", cnt2)\n",
    "print(\"Cities:\", cities2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1138,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No of unique cities: 274\nCities: ['Henderson', 'Manchester', 'Elizabeth', 'Newark', 'Paterson', 'Jersey City', 'Albuquerque', 'Buffalo', 'Rochester', 'Syracuse', 'New York', 'Yonkers', 'Charlotte', 'Winston–Salem', 'High Point', 'Greensboro', 'Fayetteville', 'Durham', 'Cary', 'Raleigh', 'Wilmington', 'Fargo', 'Cincinnati', 'Dayton', 'Toledo', 'Columbus', 'Cleveland', 'Akron', 'Oklahoma City', 'Norman', 'Tulsa', 'Broken Arrow', 'Eugene', 'Salem', 'Portland', 'Gresham', 'Erie', 'Pittsburgh', 'Allentown', 'Philadelphia', 'Providence', 'Columbia', 'Charleston', 'Sioux Falls', 'Memphis', 'Clarksville', 'Nashville', 'Murfreesboro', 'Chattanooga', 'Knoxville', 'El Paso', 'Odessa', 'Midland', 'Lubbock', 'Amarillo', 'Abilene', 'Laredo', 'Wichita Falls', 'Killeen', 'Round Rock', 'Brownsville', 'Fort Worth', 'Waco', 'Corpus Christi', 'Denton', 'Arlington', 'Grand Prairie', 'Irving', 'Carrollton', 'Frisco', 'Dallas', 'Plano', 'Richardson', 'McKinney', 'Garland', 'Mesquite', 'Houston', 'Pasadena', 'Beaumont', 'West Valley City', 'West Jordan', 'Salt Lake City', 'Provo', 'Richmond', 'Alexandria', 'Newport News', 'Chesapeake', 'Hampton', 'Norfolk', 'Virginia Beach', 'Vancouver', 'Tacoma', 'Seattle', 'Kent', 'Everett', 'Bellevue', 'Spokane', 'Madison', 'Green Bay', 'Milwaukee', 'Costa Mesa', 'West Covina', 'Santa Ana', 'Orange', 'Irvine', 'Pomona', 'Anaheim', 'Ontario', 'Rancho Cucamonga', 'Corona', 'Fontana', 'Riverside', 'Rialto', 'Victorville', 'Oceanside', 'San Bernardino', 'Carlsbad', 'Moreno Valley', 'Murrieta', 'San Diego', 'Temecula', 'Escondido', 'Chula Vista', 'El Cajon', 'Lakewood', 'Arvada', 'Fort Collins', 'Westminster', 'Thornton', 'Denver', 'Centennial', 'Colorado Springs', 'Pueblo', 'Aurora', 'Stamford', 'Bridgeport', 'Atlanta', 'Athens', 'Augusta', 'Savannah', 'Honolulu', 'Boise', 'Springfield', 'Peoria', 'Rockford', 'Elgin', 'Naperville', 'Joliet', 'Chicago', 'Evansville', 'South Bend', 'Indianapolis', 'Fort Wayne', 'Des Moines', 'Cedar Rapids', 'Davenport', 'Wichita', 'Topeka', 'Olathe', 'Kansas City', 'Overland Park', 'Louisville', 'Lexington', 'Shreveport', 'Lafayette', 'Baton Rouge', 'New Orleans', 'Baltimore', 'Worcester', 'Lowell', 'Cambridge', 'Boston', 'Grand Rapids', 'Lansing', 'Ann Arbor', 'Flint', 'Detroit', 'Sterling Heights', 'Warren', 'Minneapolis', 'Saint Paul', 'Jackson', 'Independence', 'St. Louis', 'Billings', 'Lincoln', 'Omaha', 'Reno', 'Las Vegas', 'North Las Vegas', 'San Antonio', 'McAllen', 'Austin', 'Mobile', 'Birmingham', 'Huntsville', 'Montgomery', 'Anchorage', 'Surprise', 'Glendale', 'Phoenix', 'Tempe', 'Chandler', 'Scottsdale', 'Gilbert', 'Mesa', 'Tucson', 'Little Rock', 'San Francisco', 'Santa Rosa', 'Daly City', 'Berkeley', 'Vallejo', 'Oakland', 'Hayward', 'Fairfield', 'Sunnyvale', 'Concord', 'Santa Clara', 'Fremont', 'San Jose', 'Antioch', 'Salinas', 'Sacramento', 'Elk Grove', 'Stockton', 'Roseville', 'Modesto', 'Santa Maria', 'Fresno', 'Visalia', 'San Buenaventura (Ventura)', 'Oxnard', 'Bakersfield', 'Thousand Oaks', 'Simi Valley', 'Santa Clarita', 'Los Angeles', 'Inglewood', 'Torrance', 'Burbank', 'Lancaster', 'Long Beach', 'Downey', 'Palmdale', 'Norwalk', 'El Monte', 'Huntington Beach', 'Garden Grove', 'Fullerton', 'Waterbury', 'New Haven', 'Hartford', 'Washington', 'Tallahassee', 'Clearwater', 'Saint Petersburg', 'Tampa', 'Gainesville', 'Cape Coral', 'Jacksonville', 'Orlando', 'Palm Bay', 'Port Saint Lucie', 'Pembroke Pines', 'Miramar', 'Hialeah', 'Coral Springs', 'Miami Gardens', 'Miami', 'Hollywood', 'Fort Lauderdale', 'Pompano Beach', 'West Palm Beach']\n"
     ]
    }
   ],
   "source": [
    "# Check for unique cities in pop data\n",
    "cnt1 = 0\n",
    "cities1 = []\n",
    "for i in range(0, len(df)):\n",
    "    if df['City'][i] not in cities1:\n",
    "        cities1.append(df['City'][i])\n",
    "        cnt1 += 1\n",
    "\n",
    "print(\"No of unique cities:\", cnt1)\n",
    "print(\"Cities:\", cities1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1139,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "51\n"
     ]
    }
   ],
   "source": [
    "Pacific = ['Washington', 'Oregon', 'California', 'Hawaii', 'Alaska']\n",
    "Mountain = ['Montana', 'Idaho', 'Wyoming', 'Colorado', 'Utah', 'Nevada', 'Arizona', 'New Mexico']\n",
    "WestNorthCentral = ['North Dakota', 'South Dakota', 'Nebraska', 'Kansas', 'Minnesota', 'Iowa', 'Missouri']\n",
    "WestSouthCentral = ['Texas', 'Oklahoma', 'Arkansas', 'Louisiana']\n",
    "EastNorthCentral = ['Michigan', 'Wisconsin', 'Illinois', 'Indiana', 'Ohio']\n",
    "EastSouthCentral = ['Kentucky', 'Tennessee', 'Mississippi', 'Alabama']\n",
    "SouthAtlantic = ['Florida', 'Georgia', 'South Carolina', 'North Carolina', 'Virginia', 'West Virginia', 'Maryland', 'Delaware', 'District of Columbia']\n",
    "MidAtlantic = ['Pennsylvania', 'New York', 'New Jersey']\n",
    "NorthAtlantic = ['Maine', 'Vermont', 'New Hampshire', 'Massachusetts', 'Connecticut', 'Rhode Island']\n",
    "\n",
    "statecount = len(Pacific) + len(Mountain) + len(WestNorthCentral) + len(WestSouthCentral) + len(EastNorthCentral) + len(EastSouthCentral) + len(SouthAtlantic) + len(MidAtlantic) + len(NorthAtlantic)\n",
    "\n",
    "print(statecount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1140,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[['Washington', 'Oregon', 'California', 'Hawaii', 'Alaska'], ['Montana', 'Idaho', 'Wyoming', 'Colorado', 'Utah', 'Nevada', 'Arizona', 'New Mexico'], ['North Dakota', 'South Dakota', 'Nebraska', 'Kansas', 'Minnesota', 'Iowa', 'Missouri'], ['Texas', 'Oklahoma', 'Arkansas', 'Louisiana'], ['Michigan', 'Wisconsin', 'Illinois', 'Indiana', 'Ohio'], ['Kentucky', 'Tennessee', 'Mississippi', 'Alabama'], ['Florida', 'Georgia', 'South Carolina', 'North Carolina', 'Virginia', 'West Virginia', 'Maryland', 'Delaware', 'District of Columbia'], ['Pennsylvania', 'New York', 'New Jersey'], ['Maine', 'Vermont', 'New Hampshire', 'Massachusetts', 'Connecticut', 'Rhode Island']]\n"
     ]
    }
   ],
   "source": [
    "regions = [Pacific, Mountain, WestNorthCentral, WestSouthCentral, EastNorthCentral, EastSouthCentral, SouthAtlantic, MidAtlantic, NorthAtlantic]\n",
    "print(regions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to datetime and fill\n",
    "# Add boolean column indicating interpolated row\n",
    "# Weigh city temp entries by population versus entire region\n",
    "# Add missing dates to cities with temperature data in df2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1142,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "No of unique stations: 39\nStations: ['KATL', 'KBDL', 'KBNA', 'KBOI', 'KBOS', 'KBUF', 'KBUR', 'KBWI', 'KCMH', 'KCQT', 'KCVG', 'KDCA', 'KDEN', 'KDFW', 'KDTW', 'KFAT', 'KGEG', 'KIAD', 'KIAH', 'KLAS', 'KLGA', 'KLIT', 'KMEM', 'KMSP', 'KMSY', 'KORD', 'KPDX', 'KPHL', 'KPHX', 'KPIT', 'KPWM', 'KRDU', 'KRIC', 'KSAC', 'KSEA', 'KSFO', 'KSLC', 'KSTL', 'KALB']\n"
     ]
    }
   ],
   "source": [
    "# Check for duplicate city names with different station code - 39 stations 38 cities # KPDX Portland Oregon and KPWM Portland Maine\n",
    "cnt3 = 0\n",
    "stations = []\n",
    "for i in range(0, len(df2)):\n",
    "    if df2['station_code'][i] not in stations:\n",
    "        stations.append(df2['station_code'][i])\n",
    "        cnt3 += 1\n",
    "\n",
    "print(\"No of unique stations:\", cnt3)\n",
    "print(\"Stations:\", stations)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign states to stations\n",
    "df2.loc[df2['station_code']== 'KBDL', ['state']] = 'Connecticut'\n",
    "df2.loc[df2['station_code']== 'KATL', ['state']] = 'Georgia'\n",
    "df2.loc[df2['station_code']== 'KBNA', ['state']] = 'Tennessee'\n",
    "df2.loc[df2['station_code']== 'KBOI', ['state']] = 'Idaho'\n",
    "df2.loc[df2['station_code']== 'KBOS', ['state']] = 'Massachusetts'\n",
    "df2.loc[df2['station_code']== 'KBUF', ['state']] = 'New York'\n",
    "df2.loc[df2['station_code']== 'KBUR', ['state']] = 'California'\n",
    "df2.loc[df2['station_code']== 'KBWI', ['state']] = 'Maryland'\n",
    "df2.loc[df2['station_code']== 'KCMH', ['state']] = 'Ohio'\n",
    "df2.loc[df2['station_code']== 'KCQT', ['state']] = 'California'\n",
    "df2.loc[df2['station_code']== 'KCVG', ['state']] = 'Kentucky'\n",
    "df2.loc[df2['station_code']== 'KDCA', ['state']] = 'Virginia'\n",
    "df2.loc[df2['station_code']== 'KDEN', ['state']] = 'Colorado'\n",
    "df2.loc[df2['station_code']== 'KDFW', ['state']] = 'Texas'\n",
    "df2.loc[df2['station_code']== 'KDTW', ['state']] = 'Michigan'\n",
    "df2.loc[df2['station_code']== 'KFAT', ['state']] = 'California'\n",
    "df2.loc[df2['station_code']== 'KGEG', ['state']] = 'Washington'\n",
    "df2.loc[df2['station_code']== 'KIAD', ['state']] = 'Virginia'\n",
    "df2.loc[df2['station_code']== 'KIAH', ['state']] = 'Texas'\n",
    "df2.loc[df2['station_code']== 'KLAS', ['state']] = 'Nevada'\n",
    "df2.loc[df2['station_code']== 'KLGA', ['state']] = 'New York'\n",
    "df2.loc[df2['station_code']== 'KLIT', ['state']] = 'Arkansas'\n",
    "df2.loc[df2['station_code']== 'KMEM', ['state']] = 'Tennessee'\n",
    "df2.loc[df2['station_code']== 'KMSP', ['state']] = 'Minnesota'\n",
    "df2.loc[df2['station_code']== 'KMSY', ['state']] = 'Louisiana'\n",
    "df2.loc[df2['station_code']== 'KORD', ['state']] = 'Illinois'\n",
    "df2.loc[df2['station_code']== 'KPDX', ['state']] = 'Oregon'\n",
    "df2.loc[df2['station_code']== 'KPHL', ['state']] = 'Pennsylvania'\n",
    "df2.loc[df2['station_code']== 'KPHX', ['state']] = 'Arizona'\n",
    "df2.loc[df2['station_code']== 'KPIT', ['state']] = 'Pennsylvania'\n",
    "df2.loc[df2['station_code']== 'KPWM', ['state']] = 'Maine'\n",
    "df2.loc[df2['station_code']== 'KRDU', ['state']] = 'North Carolina'\n",
    "df2.loc[df2['station_code']== 'KRIC', ['state']] = 'Virginia'\n",
    "df2.loc[df2['station_code']== 'KSAC', ['state']] = 'California'\n",
    "df2.loc[df2['station_code']== 'KSEA', ['state']] = 'Washington'\n",
    "df2.loc[df2['station_code']== 'KSFO', ['state']] = 'California'\n",
    "df2.loc[df2['station_code']== 'KSLC', ['state']] = 'Utah'\n",
    "df2.loc[df2['station_code']== 'KSTL', ['state']] = 'Missouri'\n",
    "df2.loc[df2['station_code']== 'KALB', ['state']] = 'New York'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1144,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# Assign states to regions in temperature dataset\n",
    "\n",
    "region = []\n",
    "for i in range(0, len(df2)):\n",
    "    for j in range(0, len(regions)):\n",
    "        if df2['state'][i] in regions[j]:\n",
    "            region.append(j)\n",
    "# Establish region column in temp table      \n",
    "df2[\"region\"] = region  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1145,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assign states to regions in population dataset\n",
    "\n",
    "region2 = []\n",
    "for k in range(0, len(df)):\n",
    "    for l in range(0, len(regions)):\n",
    "        if df['State'][k] in regions[l]:\n",
    "            region2.append(l)\n",
    "\n",
    "# Establish region column in pop table          \n",
    "df[\"region\"] = region2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 902,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(path_or_buf='../SynMax/Temp Data.csv')\n",
    "df.to_csv(path_or_buf='../SynMax/Pop Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1149,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert date to datetime and fill\n",
    "datetime = pd.to_datetime(df2['location_date'])\n",
    "df2['date'] = datetime.dt.date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "s=pd.date_range(df2.date.min(),df2.date.max(),freq='1 D')\n",
    "\n",
    "df3=df2.set_index(['name','station_code','state','region','date']).temp_mean_c.unstack().reindex(columns=s,fill_value='nan').stack().reset_index()\n",
    "df3.columns.values[4] = 'date'\n",
    "df3.columns.values[5] = 'temp_mean_c'\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 954,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0       2021-04-20\n75      2021-04-19\n78      2021-04-18\n153     2021-04-17\n156     2021-04-16\n           ...    \n89025   2015-01-05\n89100   2015-01-04\n89103   2015-01-03\n89178   2015-01-02\n89181   2015-01-01\nName: date, Length: 2287, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "print(df2.loc[df2['station_code'] == stations[0], 'date' ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 970,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    " missingdates = []\n",
    "#loop through each station\n",
    "for i in range(0, 1):\n",
    "    missing = []\n",
    "    missing = pd.date_range(start = '2015-01-01', end = '2021-04-20' ).difference(df2.loc[df2['station_code']== stations[i], 'date'])\n",
    "\n",
    "print(missingdates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 960,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "error",
     "ename": "Error",
     "evalue": "connect ECONNREFUSED 127.0.0.1:53724",
     "traceback": [
      "Error: connect ECONNREFUSED 127.0.0.1:53724",
      "at TCPConnectWrap.afterConnect [as oncomplete] (net.js:1146:16)"
     ]
    }
   ],
   "source": [
    "\n",
    "#loop through each station\n",
    "for i in range(0, len(stations)):\n",
    "    missingdates = []\n",
    "    missingdates = pd.date_range(start = '2015-01-01', end = '2021-04-20' ).difference(df2.loc[df2['station_code']== stations[i], 'date'])\n",
    "    #loop through each date in s\n",
    "    for j in range(0, len(s)):\n",
    "    #check/create dates in individual station\n",
    "        if s[j] not in df2.loc[df2['station_code'] == stations[0], 'date']:\n",
    "            missingdates[\"Station\"].append(stations[0])\n",
    "            missingdates[\"Date\"].append(s[j].isoformat())\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(missingdates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-922-87cb3b6a664a>, line 6)",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-922-87cb3b6a664a>\"\u001b[1;36m, line \u001b[1;32m6\u001b[0m\n\u001b[1;33m    if s[j] not in df2.loc[df2['station_code'] == stations[i]]\u001b[0m\n\u001b[1;37m                                                              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#loop through each station\n",
    "for i in range(0, len(stations)):\n",
    "    #loop through each date in s\n",
    "    for j in range(0, len(s)):\n",
    "    #check/create dates in individual station\n",
    "        if s[j] not in df2.loc[df2['station_code'] == stations[i], 'date']\n",
    "            df3=df2.set_index(['name','station_code','state','region','date']).temp_mean_c.unstack().reindex(columns=s,fill_value='nan').stack().reset_index()\n",
    "df3.columns.values[4] = 'date'\n",
    "df3.columns.values[5] = 'temp_mean_c'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 914,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "Timestamp('2015-01-01 00:00:00', freq='D')"
      ]
     },
     "metadata": {},
     "execution_count": 914
    }
   ],
   "source": [
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 961,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0       2021-04-20\n",
       "75      2021-04-19\n",
       "78      2021-04-18\n",
       "153     2021-04-17\n",
       "156     2021-04-16\n",
       "           ...    \n",
       "89025   2015-01-05\n",
       "89100   2015-01-04\n",
       "89103   2015-01-03\n",
       "89178   2015-01-02\n",
       "89181   2015-01-01\n",
       "Name: date, Length: 2287, dtype: datetime64[ns]"
      ]
     },
     "metadata": {},
     "execution_count": 961
    }
   ],
   "source": [
    "df2.loc[df2['station_code']== stations[0], 'date'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "585\n"
     ]
    }
   ],
   "source": [
    "# Calculate total number of missing days by station code - KPHX has no missing days \n",
    "missingCount = 0\n",
    "\n",
    "for i in range(0, len(stations)):\n",
    "    ct1 = len(pd.date_range(start = '2015-01-01', end = '2021-04-20' ).difference(df2.loc[df2['station_code']== stations[i], 'date']))\n",
    "    missingCount += ct1\n",
    "\n",
    "print(missingCount)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 964,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "metadata": {},
     "execution_count": 964
    }
   ],
   "source": [
    "len(pd.date_range(start = '2015-01-01', end = '2021-04-20' ).difference(df2.loc[df2['station_code']== stations[0], 'date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1014,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "DatetimeIndex(['2015-03-08', '2015-11-01', '2016-03-13', '2016-11-06',\n               '2017-03-12', '2017-09-16', '2017-11-05', '2018-03-11',\n               '2018-11-04', '2018-12-12', '2019-03-10', '2019-11-03',\n               '2020-03-08', '2020-11-01', '2021-03-14'],\n              dtype='datetime64[ns]', freq=None)\n"
     ]
    }
   ],
   "source": [
    "print(pd.date_range(start = '2015-01-01', end = '2021-04-20' ).difference(df2.loc[df2['station_code']== stations[0], 'date']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1054,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "2015-03-08 00:00:00\n"
     ]
    }
   ],
   "source": [
    "missingFromStation = pd.date_range(start = '2015-01-01', end = '2021-04-20' ).difference(df2.loc[df2['station_code']== stations[0], 'date'])\n",
    "\n",
    "print(missingFromStation[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1068,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "          date station_code\n",
      "0   2015-03-08         KATL\n",
      "1   2015-11-01         KATL\n",
      "2   2016-03-13         KATL\n",
      "3   2016-11-06         KATL\n",
      "4   2017-03-12         KATL\n",
      "5   2017-09-16         KATL\n",
      "6   2017-11-05         KATL\n",
      "7   2018-03-11         KATL\n",
      "8   2018-11-04         KATL\n",
      "9   2018-12-12         KATL\n",
      "10  2019-03-10         KATL\n",
      "11  2019-11-03         KATL\n",
      "12  2020-03-08         KATL\n",
      "13  2020-11-01         KATL\n",
      "14  2021-03-14         KATL\n",
      "89234\n",
      "          date station_code\n",
      "0   2015-03-08         KBDL\n",
      "1   2015-11-01         KBDL\n",
      "2   2016-03-13         KBDL\n",
      "3   2016-11-06         KBDL\n",
      "4   2017-03-12         KBDL\n",
      "5   2017-09-16         KBDL\n",
      "6   2017-11-05         KBDL\n",
      "7   2018-03-11         KBDL\n",
      "8   2018-11-04         KBDL\n",
      "9   2019-03-10         KBDL\n",
      "10  2019-11-03         KBDL\n",
      "11  2020-03-08         KBDL\n",
      "12  2020-11-01         KBDL\n",
      "13  2021-03-14         KBDL\n",
      "89233\n",
      "          date station_code\n",
      "0   2015-03-08         KBNA\n",
      "1   2015-11-01         KBNA\n",
      "2   2016-03-13         KBNA\n",
      "3   2016-11-06         KBNA\n",
      "4   2017-03-12         KBNA\n",
      "5   2017-09-16         KBNA\n",
      "6   2017-11-05         KBNA\n",
      "7   2018-03-11         KBNA\n",
      "8   2018-11-04         KBNA\n",
      "9   2019-03-10         KBNA\n",
      "10  2019-11-03         KBNA\n",
      "11  2020-03-08         KBNA\n",
      "12  2020-11-01         KBNA\n",
      "13  2021-03-14         KBNA\n",
      "89233\n",
      "          date station_code\n",
      "0   2015-03-08         KBOI\n",
      "1   2015-11-01         KBOI\n",
      "2   2016-03-13         KBOI\n",
      "3   2016-11-06         KBOI\n",
      "4   2017-03-12         KBOI\n",
      "5   2017-09-16         KBOI\n",
      "6   2017-11-05         KBOI\n",
      "7   2018-03-11         KBOI\n",
      "8   2018-11-04         KBOI\n",
      "9   2019-03-10         KBOI\n",
      "10  2019-11-03         KBOI\n",
      "11  2020-03-08         KBOI\n",
      "12  2020-11-01         KBOI\n",
      "13  2021-03-14         KBOI\n",
      "89233\n",
      "          date station_code\n",
      "0   2015-03-08         KBOS\n",
      "1   2015-11-01         KBOS\n",
      "2   2016-03-13         KBOS\n",
      "3   2016-11-06         KBOS\n",
      "4   2017-03-12         KBOS\n",
      "5   2017-09-16         KBOS\n",
      "6   2017-11-05         KBOS\n",
      "7   2018-03-11         KBOS\n",
      "8   2018-11-04         KBOS\n",
      "9   2019-03-10         KBOS\n",
      "10  2019-11-03         KBOS\n",
      "11  2020-03-08         KBOS\n",
      "12  2020-11-01         KBOS\n",
      "13  2021-03-14         KBOS\n",
      "89233\n",
      "          date station_code\n",
      "0   2015-03-08         KBUF\n",
      "1   2015-11-01         KBUF\n",
      "2   2016-03-13         KBUF\n",
      "3   2016-11-06         KBUF\n",
      "4   2017-03-12         KBUF\n",
      "5   2017-09-16         KBUF\n",
      "6   2017-11-05         KBUF\n",
      "7   2018-03-11         KBUF\n",
      "8   2018-11-04         KBUF\n",
      "9   2019-03-10         KBUF\n",
      "10  2019-11-03         KBUF\n",
      "11  2020-03-08         KBUF\n",
      "12  2020-05-28         KBUF\n",
      "13  2020-11-01         KBUF\n",
      "14  2021-03-14         KBUF\n",
      "15  2021-04-12         KBUF\n",
      "89235\n",
      "          date station_code\n",
      "0   2015-03-08         KBUR\n",
      "1   2015-09-17         KBUR\n",
      "2   2015-11-01         KBUR\n",
      "3   2016-03-13         KBUR\n",
      "4   2016-11-06         KBUR\n",
      "5   2017-03-12         KBUR\n",
      "6   2017-09-16         KBUR\n",
      "7   2017-11-05         KBUR\n",
      "8   2018-03-11         KBUR\n",
      "9   2018-11-04         KBUR\n",
      "10  2019-03-10         KBUR\n",
      "11  2019-11-03         KBUR\n",
      "12  2020-03-08         KBUR\n",
      "13  2020-11-01         KBUR\n",
      "14  2021-03-14         KBUR\n",
      "89234\n",
      "          date station_code\n",
      "0   2015-03-08         KBWI\n",
      "1   2015-04-17         KBWI\n",
      "2   2015-09-17         KBWI\n",
      "3   2015-11-01         KBWI\n",
      "4   2016-03-13         KBWI\n",
      "5   2016-11-06         KBWI\n",
      "6   2017-03-12         KBWI\n",
      "7   2017-09-16         KBWI\n",
      "8   2017-11-05         KBWI\n",
      "9   2018-03-11         KBWI\n",
      "10  2018-11-04         KBWI\n",
      "11  2019-03-10         KBWI\n",
      "12  2019-11-03         KBWI\n",
      "13  2020-03-08         KBWI\n",
      "14  2020-11-01         KBWI\n",
      "15  2021-03-14         KBWI\n",
      "89235\n",
      "          date station_code\n",
      "0   2015-03-08         KCMH\n",
      "1   2015-09-17         KCMH\n",
      "2   2015-11-01         KCMH\n",
      "3   2016-03-13         KCMH\n",
      "4   2016-11-06         KCMH\n",
      "5   2017-03-12         KCMH\n",
      "6   2017-09-16         KCMH\n",
      "7   2017-11-05         KCMH\n",
      "8   2018-03-11         KCMH\n",
      "9   2018-11-04         KCMH\n",
      "10  2019-03-10         KCMH\n",
      "11  2019-11-03         KCMH\n",
      "12  2020-03-08         KCMH\n",
      "13  2020-11-01         KCMH\n",
      "14  2021-03-14         KCMH\n",
      "89234\n",
      "          date station_code\n",
      "0   2015-03-08         KCQT\n",
      "1   2015-09-17         KCQT\n",
      "2   2015-11-01         KCQT\n",
      "3   2016-03-13         KCQT\n",
      "4   2016-11-06         KCQT\n",
      "5   2017-03-12         KCQT\n",
      "6   2017-09-16         KCQT\n",
      "7   2017-11-05         KCQT\n",
      "8   2018-03-11         KCQT\n",
      "9   2018-11-04         KCQT\n",
      "10  2019-03-10         KCQT\n",
      "11  2019-11-03         KCQT\n",
      "12  2020-03-08         KCQT\n",
      "13  2020-11-01         KCQT\n",
      "14  2021-03-14         KCQT\n",
      "89234\n",
      "          date station_code\n",
      "0   2015-03-08         KCVG\n",
      "1   2015-09-17         KCVG\n",
      "2   2015-11-01         KCVG\n",
      "3   2016-03-13         KCVG\n",
      "4   2016-11-06         KCVG\n",
      "5   2017-03-12         KCVG\n",
      "6   2017-09-16         KCVG\n",
      "7   2017-11-05         KCVG\n",
      "8   2018-03-11         KCVG\n",
      "9   2018-11-04         KCVG\n",
      "10  2019-03-10         KCVG\n",
      "11  2019-11-03         KCVG\n",
      "12  2020-03-08         KCVG\n",
      "13  2020-11-01         KCVG\n",
      "14  2021-03-14         KCVG\n",
      "89234\n",
      "          date station_code\n",
      "0   2015-03-08         KDCA\n",
      "1   2015-09-17         KDCA\n",
      "2   2015-11-01         KDCA\n",
      "3   2016-03-13         KDCA\n",
      "4   2016-11-06         KDCA\n",
      "5   2017-03-12         KDCA\n",
      "6   2017-09-16         KDCA\n",
      "7   2017-11-05         KDCA\n",
      "8   2018-03-11         KDCA\n",
      "9   2018-11-04         KDCA\n",
      "10  2019-03-10         KDCA\n",
      "11  2019-11-03         KDCA\n",
      "12  2020-03-08         KDCA\n",
      "13  2020-11-01         KDCA\n",
      "14  2021-03-14         KDCA\n",
      "89234\n",
      "          date station_code\n",
      "0   2015-03-08         KDEN\n",
      "1   2015-11-01         KDEN\n",
      "2   2016-03-13         KDEN\n",
      "3   2016-11-06         KDEN\n",
      "4   2017-03-12         KDEN\n",
      "5   2017-09-16         KDEN\n",
      "6   2017-10-21         KDEN\n",
      "7   2017-11-05         KDEN\n",
      "8   2018-03-11         KDEN\n",
      "9   2018-11-04         KDEN\n",
      "10  2019-03-10         KDEN\n",
      "11  2019-11-03         KDEN\n",
      "12  2020-03-08         KDEN\n",
      "13  2020-07-13         KDEN\n",
      "14  2020-11-01         KDEN\n",
      "15  2021-03-14         KDEN\n",
      "89235\n",
      "          date station_code\n",
      "0   2015-03-08         KDFW\n",
      "1   2015-09-17         KDFW\n",
      "2   2015-11-01         KDFW\n",
      "3   2016-03-13         KDFW\n",
      "4   2016-11-06         KDFW\n",
      "5   2017-03-12         KDFW\n",
      "6   2017-09-16         KDFW\n",
      "7   2017-11-05         KDFW\n",
      "8   2018-03-11         KDFW\n",
      "9   2018-11-04         KDFW\n",
      "10  2019-03-10         KDFW\n",
      "11  2019-11-03         KDFW\n",
      "12  2020-03-08         KDFW\n",
      "13  2020-05-28         KDFW\n",
      "14  2020-11-01         KDFW\n",
      "15  2021-03-14         KDFW\n",
      "89235\n",
      "          date station_code\n",
      "0   2015-03-08         KDTW\n",
      "1   2015-11-01         KDTW\n",
      "2   2016-03-13         KDTW\n",
      "3   2016-11-06         KDTW\n",
      "4   2017-03-12         KDTW\n",
      "5   2017-09-16         KDTW\n",
      "6   2017-11-05         KDTW\n",
      "7   2018-03-11         KDTW\n",
      "8   2018-11-04         KDTW\n",
      "9   2019-03-10         KDTW\n",
      "10  2019-11-03         KDTW\n",
      "11  2019-11-28         KDTW\n",
      "12  2020-03-08         KDTW\n",
      "13  2020-05-28         KDTW\n",
      "14  2020-11-01         KDTW\n",
      "15  2021-03-14         KDTW\n",
      "89235\n",
      "          date station_code\n",
      "0   2015-03-08         KFAT\n",
      "1   2015-11-01         KFAT\n",
      "2   2016-03-13         KFAT\n",
      "3   2016-11-06         KFAT\n",
      "4   2017-03-12         KFAT\n",
      "5   2017-09-16         KFAT\n",
      "6   2017-11-05         KFAT\n",
      "7   2018-03-11         KFAT\n",
      "8   2018-11-04         KFAT\n",
      "9   2019-03-10         KFAT\n",
      "10  2019-11-03         KFAT\n",
      "11  2019-11-28         KFAT\n",
      "12  2020-03-08         KFAT\n",
      "13  2020-11-01         KFAT\n",
      "14  2021-03-14         KFAT\n",
      "89234\n",
      "          date station_code\n",
      "0   2015-03-08         KGEG\n",
      "1   2015-11-01         KGEG\n",
      "2   2016-03-13         KGEG\n",
      "3   2016-11-06         KGEG\n",
      "4   2017-03-12         KGEG\n",
      "5   2017-09-16         KGEG\n",
      "6   2017-11-05         KGEG\n",
      "7   2018-03-11         KGEG\n",
      "8   2018-11-04         KGEG\n",
      "9   2019-03-10         KGEG\n",
      "10  2019-11-03         KGEG\n",
      "11  2019-11-28         KGEG\n",
      "12  2020-03-08         KGEG\n",
      "13  2020-11-01         KGEG\n",
      "14  2021-03-14         KGEG\n",
      "89234\n",
      "          date station_code\n",
      "0   2015-03-08         KIAD\n",
      "1   2015-11-01         KIAD\n",
      "2   2016-03-13         KIAD\n",
      "3   2016-11-06         KIAD\n",
      "4   2017-03-12         KIAD\n",
      "5   2017-09-16         KIAD\n",
      "6   2017-11-05         KIAD\n",
      "7   2018-03-11         KIAD\n",
      "8   2018-11-04         KIAD\n",
      "9   2019-03-10         KIAD\n",
      "10  2019-11-03         KIAD\n",
      "11  2019-11-28         KIAD\n",
      "12  2020-03-08         KIAD\n",
      "13  2020-11-01         KIAD\n",
      "14  2021-03-14         KIAD\n",
      "89234\n",
      "          date station_code\n",
      "0   2015-03-08         KIAH\n",
      "1   2015-11-01         KIAH\n",
      "2   2016-03-13         KIAH\n",
      "3   2016-11-06         KIAH\n",
      "4   2017-03-12         KIAH\n",
      "5   2017-09-16         KIAH\n",
      "6   2017-11-05         KIAH\n",
      "7   2018-03-11         KIAH\n",
      "8   2018-11-04         KIAH\n",
      "9   2019-03-10         KIAH\n",
      "10  2019-11-03         KIAH\n",
      "11  2019-11-28         KIAH\n",
      "12  2020-03-08         KIAH\n",
      "13  2020-11-01         KIAH\n",
      "14  2021-03-14         KIAH\n",
      "89234\n",
      "          date station_code\n",
      "0   2015-03-08         KLAS\n",
      "1   2015-11-01         KLAS\n",
      "2   2016-03-13         KLAS\n",
      "3   2016-11-06         KLAS\n",
      "4   2017-03-12         KLAS\n",
      "5   2017-09-16         KLAS\n",
      "6   2017-11-05         KLAS\n",
      "7   2018-03-11         KLAS\n",
      "8   2018-11-04         KLAS\n",
      "9   2019-03-10         KLAS\n",
      "10  2019-11-03         KLAS\n",
      "11  2020-03-08         KLAS\n",
      "12  2020-11-01         KLAS\n",
      "13  2021-03-14         KLAS\n",
      "89233\n",
      "          date station_code\n",
      "0   2015-03-08         KLGA\n",
      "1   2015-11-01         KLGA\n",
      "2   2016-03-13         KLGA\n",
      "3   2016-11-06         KLGA\n",
      "4   2017-03-12         KLGA\n",
      "5   2017-09-16         KLGA\n",
      "6   2017-11-05         KLGA\n",
      "7   2018-03-11         KLGA\n",
      "8   2018-11-04         KLGA\n",
      "9   2019-03-10         KLGA\n",
      "10  2019-11-03         KLGA\n",
      "11  2020-03-08         KLGA\n",
      "12  2020-11-01         KLGA\n",
      "13  2021-03-14         KLGA\n",
      "89233\n",
      "          date station_code\n",
      "0   2015-03-08         KLIT\n",
      "1   2015-11-01         KLIT\n",
      "2   2016-03-13         KLIT\n",
      "3   2016-11-06         KLIT\n",
      "4   2017-03-12         KLIT\n",
      "5   2017-09-16         KLIT\n",
      "6   2017-11-05         KLIT\n",
      "7   2018-03-11         KLIT\n",
      "8   2018-11-04         KLIT\n",
      "9   2019-03-10         KLIT\n",
      "10  2019-11-03         KLIT\n",
      "11  2020-03-08         KLIT\n",
      "12  2020-11-01         KLIT\n",
      "13  2021-03-14         KLIT\n",
      "89233\n",
      "          date station_code\n",
      "0   2015-03-08         KMEM\n",
      "1   2015-11-01         KMEM\n",
      "2   2016-03-13         KMEM\n",
      "3   2016-11-06         KMEM\n",
      "4   2017-03-12         KMEM\n",
      "5   2017-09-16         KMEM\n",
      "6   2017-11-05         KMEM\n",
      "7   2018-03-11         KMEM\n",
      "8   2018-11-04         KMEM\n",
      "9   2019-03-10         KMEM\n",
      "10  2019-11-03         KMEM\n",
      "11  2020-03-08         KMEM\n",
      "12  2020-11-01         KMEM\n",
      "13  2021-03-14         KMEM\n",
      "89233\n",
      "          date station_code\n",
      "0   2015-03-08         KMSP\n",
      "1   2015-11-01         KMSP\n",
      "2   2016-03-13         KMSP\n",
      "3   2016-11-06         KMSP\n",
      "4   2017-03-12         KMSP\n",
      "5   2017-09-16         KMSP\n",
      "6   2017-11-05         KMSP\n",
      "7   2018-03-11         KMSP\n",
      "8   2018-11-04         KMSP\n",
      "9   2019-03-10         KMSP\n",
      "10  2019-11-03         KMSP\n",
      "11  2020-03-08         KMSP\n",
      "12  2020-07-13         KMSP\n",
      "13  2020-11-01         KMSP\n",
      "14  2021-03-14         KMSP\n",
      "89234\n",
      "          date station_code\n",
      "0   2015-03-08         KMSY\n",
      "1   2015-11-01         KMSY\n",
      "2   2016-03-13         KMSY\n",
      "3   2016-11-06         KMSY\n",
      "4   2017-03-12         KMSY\n",
      "5   2017-09-16         KMSY\n",
      "6   2017-11-05         KMSY\n",
      "7   2018-03-11         KMSY\n",
      "8   2018-11-04         KMSY\n",
      "9   2018-12-12         KMSY\n",
      "10  2019-03-10         KMSY\n",
      "11  2019-11-03         KMSY\n",
      "12  2020-03-08         KMSY\n",
      "13  2020-07-13         KMSY\n",
      "14  2020-11-01         KMSY\n",
      "15  2021-03-14         KMSY\n",
      "89235\n",
      "          date station_code\n",
      "0   2015-03-08         KORD\n",
      "1   2015-11-01         KORD\n",
      "2   2016-03-13         KORD\n",
      "3   2016-11-06         KORD\n",
      "4   2017-03-12         KORD\n",
      "5   2017-09-16         KORD\n",
      "6   2017-11-05         KORD\n",
      "7   2018-03-11         KORD\n",
      "8   2018-11-04         KORD\n",
      "9   2019-03-10         KORD\n",
      "10  2019-11-03         KORD\n",
      "11  2020-03-08         KORD\n",
      "12  2020-11-01         KORD\n",
      "13  2021-03-14         KORD\n",
      "89233\n",
      "          date station_code\n",
      "0   2015-03-08         KPDX\n",
      "1   2015-04-17         KPDX\n",
      "2   2015-11-01         KPDX\n",
      "3   2016-03-13         KPDX\n",
      "4   2016-11-06         KPDX\n",
      "5   2017-03-12         KPDX\n",
      "6   2017-09-16         KPDX\n",
      "7   2017-11-05         KPDX\n",
      "8   2018-03-11         KPDX\n",
      "9   2018-11-04         KPDX\n",
      "10  2019-03-10         KPDX\n",
      "11  2019-11-03         KPDX\n",
      "12  2020-03-08         KPDX\n",
      "13  2020-11-01         KPDX\n",
      "14  2021-03-14         KPDX\n",
      "89234\n",
      "          date station_code\n",
      "0   2015-03-08         KPHL\n",
      "1   2015-11-01         KPHL\n",
      "2   2016-03-13         KPHL\n",
      "3   2016-11-06         KPHL\n",
      "4   2017-03-12         KPHL\n",
      "5   2017-09-16         KPHL\n",
      "6   2017-11-05         KPHL\n",
      "7   2018-03-11         KPHL\n",
      "8   2018-11-04         KPHL\n",
      "9   2019-03-10         KPHL\n",
      "10  2019-11-03         KPHL\n",
      "11  2020-03-08         KPHL\n",
      "12  2020-11-01         KPHL\n",
      "13  2021-03-14         KPHL\n",
      "89233\n",
      "Empty DataFrame\n",
      "Columns: [date, station_code]\n",
      "Index: []\n",
      "89219\n",
      "          date station_code\n",
      "0   2015-03-08         KPIT\n",
      "1   2015-11-01         KPIT\n",
      "2   2016-03-13         KPIT\n",
      "3   2016-11-06         KPIT\n",
      "4   2017-03-12         KPIT\n",
      "5   2017-09-16         KPIT\n",
      "6   2017-11-05         KPIT\n",
      "7   2018-03-11         KPIT\n",
      "8   2018-11-04         KPIT\n",
      "9   2019-03-10         KPIT\n",
      "10  2019-11-03         KPIT\n",
      "11  2020-03-08         KPIT\n",
      "12  2020-11-01         KPIT\n",
      "13  2021-03-14         KPIT\n",
      "89233\n",
      "          date station_code\n",
      "0   2015-03-08         KPWM\n",
      "1   2015-11-01         KPWM\n",
      "2   2016-03-13         KPWM\n",
      "3   2016-11-06         KPWM\n",
      "4   2017-03-12         KPWM\n",
      "5   2017-09-16         KPWM\n",
      "6   2017-11-05         KPWM\n",
      "7   2018-03-11         KPWM\n",
      "8   2018-11-04         KPWM\n",
      "9   2019-03-10         KPWM\n",
      "10  2019-11-03         KPWM\n",
      "11  2020-03-08         KPWM\n",
      "12  2020-11-01         KPWM\n",
      "13  2021-03-14         KPWM\n",
      "89233\n",
      "          date station_code\n",
      "0   2015-03-08         KRDU\n",
      "1   2015-11-01         KRDU\n",
      "2   2016-03-13         KRDU\n",
      "3   2016-11-06         KRDU\n",
      "4   2017-03-12         KRDU\n",
      "5   2017-09-16         KRDU\n",
      "6   2017-11-05         KRDU\n",
      "7   2018-03-11         KRDU\n",
      "8   2018-11-04         KRDU\n",
      "9   2019-03-10         KRDU\n",
      "10  2019-11-03         KRDU\n",
      "11  2020-03-08         KRDU\n",
      "12  2020-11-01         KRDU\n",
      "13  2021-03-14         KRDU\n",
      "89233\n",
      "          date station_code\n",
      "0   2015-03-08         KRIC\n",
      "1   2015-11-01         KRIC\n",
      "2   2016-03-13         KRIC\n",
      "3   2016-11-06         KRIC\n",
      "4   2017-03-12         KRIC\n",
      "5   2017-09-16         KRIC\n",
      "6   2017-11-05         KRIC\n",
      "7   2018-03-11         KRIC\n",
      "8   2018-11-04         KRIC\n",
      "9   2019-03-10         KRIC\n",
      "10  2019-11-03         KRIC\n",
      "11  2020-03-08         KRIC\n",
      "12  2020-11-01         KRIC\n",
      "13  2021-03-14         KRIC\n",
      "89233\n",
      "          date station_code\n",
      "0   2015-03-08         KSAC\n",
      "1   2015-11-01         KSAC\n",
      "2   2016-03-13         KSAC\n",
      "3   2016-11-06         KSAC\n",
      "4   2017-03-12         KSAC\n",
      "5   2017-09-16         KSAC\n",
      "6   2017-11-05         KSAC\n",
      "7   2018-03-11         KSAC\n",
      "8   2018-11-04         KSAC\n",
      "9   2019-03-10         KSAC\n",
      "10  2019-11-03         KSAC\n",
      "11  2020-03-08         KSAC\n",
      "12  2020-11-01         KSAC\n",
      "13  2021-03-14         KSAC\n",
      "89233\n",
      "          date station_code\n",
      "0   2015-03-08         KSEA\n",
      "1   2015-11-01         KSEA\n",
      "2   2016-03-13         KSEA\n",
      "3   2016-11-06         KSEA\n",
      "4   2017-03-12         KSEA\n",
      "5   2017-09-16         KSEA\n",
      "6   2017-11-05         KSEA\n",
      "7   2018-03-11         KSEA\n",
      "8   2018-11-04         KSEA\n",
      "9   2019-03-10         KSEA\n",
      "10  2019-11-03         KSEA\n",
      "11  2020-03-08         KSEA\n",
      "12  2020-11-01         KSEA\n",
      "13  2021-03-14         KSEA\n",
      "89233\n",
      "          date station_code\n",
      "0   2015-03-08         KSFO\n",
      "1   2015-11-01         KSFO\n",
      "2   2016-03-13         KSFO\n",
      "3   2016-11-06         KSFO\n",
      "4   2017-03-12         KSFO\n",
      "5   2017-09-16         KSFO\n",
      "6   2017-11-05         KSFO\n",
      "7   2018-03-11         KSFO\n",
      "8   2018-11-04         KSFO\n",
      "9   2019-03-10         KSFO\n",
      "10  2019-07-06         KSFO\n",
      "11  2019-11-03         KSFO\n",
      "12  2020-03-08         KSFO\n",
      "13  2020-11-01         KSFO\n",
      "14  2021-03-14         KSFO\n",
      "89234\n",
      "          date station_code\n",
      "0   2015-03-08         KSLC\n",
      "1   2015-11-01         KSLC\n",
      "2   2016-03-13         KSLC\n",
      "3   2016-11-06         KSLC\n",
      "4   2017-03-12         KSLC\n",
      "5   2017-09-16         KSLC\n",
      "6   2017-11-05         KSLC\n",
      "7   2018-03-11         KSLC\n",
      "8   2018-11-04         KSLC\n",
      "9   2019-03-10         KSLC\n",
      "10  2019-07-06         KSLC\n",
      "11  2019-11-03         KSLC\n",
      "12  2020-03-08         KSLC\n",
      "13  2020-11-01         KSLC\n",
      "14  2021-03-14         KSLC\n",
      "89234\n",
      "          date station_code\n",
      "0   2015-03-08         KSTL\n",
      "1   2015-11-01         KSTL\n",
      "2   2016-03-13         KSTL\n",
      "3   2016-11-06         KSTL\n",
      "4   2017-03-12         KSTL\n",
      "5   2017-09-16         KSTL\n",
      "6   2017-11-05         KSTL\n",
      "7   2018-03-11         KSTL\n",
      "8   2018-11-04         KSTL\n",
      "9   2019-03-10         KSTL\n",
      "10  2019-11-03         KSTL\n",
      "11  2020-03-08         KSTL\n",
      "12  2020-11-01         KSTL\n",
      "13  2021-03-14         KSTL\n",
      "89233\n",
      "          date station_code\n",
      "0   2015-03-08         KALB\n",
      "1   2015-11-01         KALB\n",
      "2   2016-03-13         KALB\n",
      "3   2016-11-06         KALB\n",
      "4   2017-03-12         KALB\n",
      "5   2017-09-16         KALB\n",
      "6   2017-11-05         KALB\n",
      "7   2018-03-11         KALB\n",
      "8   2018-11-04         KALB\n",
      "9   2019-03-10         KALB\n",
      "10  2019-11-03         KALB\n",
      "11  2020-03-08         KALB\n",
      "12  2020-11-01         KALB\n",
      "13  2021-03-14         KALB\n",
      "14  2021-04-20         KALB\n",
      "89234\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of                 name station_code location_date  temp_mean_c  temp_min_c  \\\n",
       "0            Atlanta         KATL     4/20/2021    17.491667        12.2   \n",
       "1      Windsor Locks         KBDL     4/20/2021    13.887500         3.9   \n",
       "2          Nashville         KBNA     4/20/2021    15.175000         6.7   \n",
       "3              Boise         KBOI     4/20/2021     9.329167         1.7   \n",
       "4             Boston         KBOS     4/20/2021    16.716667        10.0   \n",
       "...              ...          ...           ...          ...         ...   \n",
       "89229            NaN         KALB           NaN          NaN         NaN   \n",
       "89230            NaN         KALB           NaN          NaN         NaN   \n",
       "89231            NaN         KALB           NaN          NaN         NaN   \n",
       "89232            NaN         KALB           NaN          NaN         NaN   \n",
       "89233            NaN         KALB           NaN          NaN         NaN   \n",
       "\n",
       "       temp_max_c          state  region                 date  \n",
       "0            23.3        Georgia     6.0  2021-04-20 00:00:00  \n",
       "1            22.2    Connecticut     8.0  2021-04-20 00:00:00  \n",
       "2            23.3      Tennessee     5.0  2021-04-20 00:00:00  \n",
       "3            15.6          Idaho     1.0  2021-04-20 00:00:00  \n",
       "4            23.9  Massachusetts     8.0  2021-04-20 00:00:00  \n",
       "...           ...            ...     ...                  ...  \n",
       "89229         NaN            NaN     NaN           2019-11-03  \n",
       "89230         NaN            NaN     NaN           2020-03-08  \n",
       "89231         NaN            NaN     NaN           2020-11-01  \n",
       "89232         NaN            NaN     NaN           2021-03-14  \n",
       "89233         NaN            NaN     NaN           2021-04-20  \n",
       "\n",
       "[89234 rows x 9 columns]>"
      ]
     },
     "metadata": {},
     "execution_count": 1068
    }
   ],
   "source": [
    "# Create loop for all stations\n",
    "missingFromStation = []\n",
    "newFrame = []\n",
    "df5 = []\n",
    "for i in range(0, len(stations)):\n",
    "\n",
    "    missingFromStation = pd.date_range(start = '2015-01-01', end = '2021-04-20' ).difference(df2.loc[df2['station_code']== stations[i], 'date'])\n",
    "# Within each station create a new frame including only missing dates and station code\n",
    "    newFrame = pd.DataFrame({'date': missingFromStation.date, 'station_code': stations[i]})\n",
    "    print(newFrame)\n",
    "# Merge new frame with df2 (ignoring index?)\n",
    "    df5 = df2.append(newFrame, ignore_index=True)\n",
    "    print(len(df5))\n",
    "\n",
    "#Use for loop to create an array of stations\n",
    "stations\n",
    "df5.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1109,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "metadata": {},
     "execution_count": 1109
    }
   ],
   "source": [
    "len(pd.date_range(start = '2015-01-01', end = '2021-04-20' ).difference(df2.loc[df2['station_code']==stations[0], 'date']))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1098,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2        KBNA\n",
       "73       KBNA\n",
       "80       KBNA\n",
       "151      KBNA\n",
       "158      KBNA\n",
       "         ... \n",
       "89027    KBNA\n",
       "89098    KBNA\n",
       "89105    KBNA\n",
       "89176    KBNA\n",
       "89183    KBNA\n",
       "Name: station_code, Length: 2288, dtype: object"
      ]
     },
     "metadata": {},
     "execution_count": 1098
    }
   ],
   "source": [
    "df2.loc[df2['station_code']== stations[2], 'station_code']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1120,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "DatetimeIndex([], dtype='datetime64[ns]', freq=None)"
      ]
     },
     "metadata": {},
     "execution_count": 1120
    }
   ],
   "source": [
    "pd.date_range(start = '2015-01-01', end = '2021-04-20' ).difference(df2.loc[df2['station_code']== stations[28], 'date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "                   name station_code location_date  temp_mean_c  temp_min_c  \\\n0               Atlanta         KATL     4/20/2021    17.491667        12.2   \n1         Windsor Locks         KBDL     4/20/2021    13.887500         3.9   \n2             Nashville         KBNA     4/20/2021    15.175000         6.7   \n3                 Boise         KBOI     4/20/2021     9.329167         1.7   \n4                Boston         KBOS     4/20/2021    16.716667        10.0   \n...                 ...          ...           ...          ...         ...   \n89214  Sacramento/Execu         KSAC      1/1/2015     3.500000        -1.7   \n89215          Portland         KPWM      1/1/2015    -3.454167        -7.8   \n89216    Raleigh/Durham         KRDU      1/1/2015     3.291667        -4.4   \n89217        Pittsburgh         KPIT      1/1/2015    -2.570833        -6.7   \n89218  Phoenix/Sky HRBR         KPHX      1/1/2015     4.354167         1.7   \n\n       temp_max_c           state  region        date  \n0            23.3         Georgia       6  2021-04-20  \n1            22.2     Connecticut       8  2021-04-20  \n2            23.3       Tennessee       5  2021-04-20  \n3            15.6           Idaho       1  2021-04-20  \n4            23.9   Massachusetts       8  2021-04-20  \n...           ...             ...     ...         ...  \n89214        10.6      California       0  2015-01-01  \n89215        -0.6           Maine       8  2015-01-01  \n89216        10.6  North Carolina       6  2015-01-01  \n89217         1.7    Pennsylvania       7  2015-01-01  \n89218         7.2         Arizona       1  2015-01-01  \n\n[89219 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df2)\n",
    "df2 = df2.set_index(\"date\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1151,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['missing'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1152,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(path_or_buf='../SynMax/Temp Data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "               name station_code location_date  temp_mean_c  temp_min_c  \\\n",
       "date                                                                      \n",
       "2021-04-20  Atlanta         KATL     4/20/2021    17.491667        12.2   \n",
       "2021-04-19  Atlanta         KATL     4/19/2021    16.154167        11.1   \n",
       "2021-04-18  Atlanta         KATL     4/18/2021    16.720833        11.7   \n",
       "2021-04-17  Atlanta         KATL     4/17/2021    16.795833        12.8   \n",
       "2021-04-16  Atlanta         KATL     4/16/2021    14.695833        10.0   \n",
       "...             ...          ...           ...          ...         ...   \n",
       "2015-01-05  Atlanta         KATL      1/5/2015     5.045833         1.1   \n",
       "2015-01-04  Atlanta         KATL      1/4/2015    14.279167         6.7   \n",
       "2015-01-03  Atlanta         KATL      1/3/2015    11.154167         8.9   \n",
       "2015-01-02  Atlanta         KATL      1/2/2015     8.116667         6.7   \n",
       "2015-01-01  Atlanta         KATL      1/1/2015     6.758333         0.0   \n",
       "\n",
       "            temp_max_c    state  region  missing  \n",
       "date                                              \n",
       "2021-04-20        23.3  Georgia       6        0  \n",
       "2021-04-19        21.7  Georgia       6        0  \n",
       "2021-04-18        22.2  Georgia       6        0  \n",
       "2021-04-17        21.7  Georgia       6        0  \n",
       "2021-04-16        19.4  Georgia       6        0  \n",
       "...                ...      ...     ...      ...  \n",
       "2015-01-05         8.9  Georgia       6        0  \n",
       "2015-01-04        17.8  Georgia       6        0  \n",
       "2015-01-03        16.1  Georgia       6        0  \n",
       "2015-01-02         9.4  Georgia       6        0  \n",
       "2015-01-01        13.9  Georgia       6        0  \n",
       "\n",
       "[2287 rows x 9 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name</th>\n      <th>station_code</th>\n      <th>location_date</th>\n      <th>temp_mean_c</th>\n      <th>temp_min_c</th>\n      <th>temp_max_c</th>\n      <th>state</th>\n      <th>region</th>\n      <th>missing</th>\n    </tr>\n    <tr>\n      <th>date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2021-04-20</th>\n      <td>Atlanta</td>\n      <td>KATL</td>\n      <td>4/20/2021</td>\n      <td>17.491667</td>\n      <td>12.2</td>\n      <td>23.3</td>\n      <td>Georgia</td>\n      <td>6</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-04-19</th>\n      <td>Atlanta</td>\n      <td>KATL</td>\n      <td>4/19/2021</td>\n      <td>16.154167</td>\n      <td>11.1</td>\n      <td>21.7</td>\n      <td>Georgia</td>\n      <td>6</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-04-18</th>\n      <td>Atlanta</td>\n      <td>KATL</td>\n      <td>4/18/2021</td>\n      <td>16.720833</td>\n      <td>11.7</td>\n      <td>22.2</td>\n      <td>Georgia</td>\n      <td>6</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-04-17</th>\n      <td>Atlanta</td>\n      <td>KATL</td>\n      <td>4/17/2021</td>\n      <td>16.795833</td>\n      <td>12.8</td>\n      <td>21.7</td>\n      <td>Georgia</td>\n      <td>6</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2021-04-16</th>\n      <td>Atlanta</td>\n      <td>KATL</td>\n      <td>4/16/2021</td>\n      <td>14.695833</td>\n      <td>10.0</td>\n      <td>19.4</td>\n      <td>Georgia</td>\n      <td>6</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2015-01-05</th>\n      <td>Atlanta</td>\n      <td>KATL</td>\n      <td>1/5/2015</td>\n      <td>5.045833</td>\n      <td>1.1</td>\n      <td>8.9</td>\n      <td>Georgia</td>\n      <td>6</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2015-01-04</th>\n      <td>Atlanta</td>\n      <td>KATL</td>\n      <td>1/4/2015</td>\n      <td>14.279167</td>\n      <td>6.7</td>\n      <td>17.8</td>\n      <td>Georgia</td>\n      <td>6</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2015-01-03</th>\n      <td>Atlanta</td>\n      <td>KATL</td>\n      <td>1/3/2015</td>\n      <td>11.154167</td>\n      <td>8.9</td>\n      <td>16.1</td>\n      <td>Georgia</td>\n      <td>6</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2015-01-02</th>\n      <td>Atlanta</td>\n      <td>KATL</td>\n      <td>1/2/2015</td>\n      <td>8.116667</td>\n      <td>6.7</td>\n      <td>9.4</td>\n      <td>Georgia</td>\n      <td>6</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2015-01-01</th>\n      <td>Atlanta</td>\n      <td>KATL</td>\n      <td>1/1/2015</td>\n      <td>6.758333</td>\n      <td>0.0</td>\n      <td>13.9</td>\n      <td>Georgia</td>\n      <td>6</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>2287 rows × 9 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 1153
    }
   ],
   "source": [
    "df2.loc[df2['station_code'] == stations[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1154,
   "metadata": {},
   "outputs": [],
   "source": [
    "KATL = df2.loc[df2['station_code'] == stations[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1230,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "           location_date  missing     name  region    state station_code  \\\n",
      "2015-01-01      1/1/2015      0.0  Atlanta     6.0  Georgia         KATL   \n",
      "2015-01-02      1/2/2015      0.0  Atlanta     6.0  Georgia         KATL   \n",
      "2015-01-03      1/3/2015      0.0  Atlanta     6.0  Georgia         KATL   \n",
      "2015-01-04      1/4/2015      0.0  Atlanta     6.0  Georgia         KATL   \n",
      "2015-01-05      1/5/2015      0.0  Atlanta     6.0  Georgia         KATL   \n",
      "...                  ...      ...      ...     ...      ...          ...   \n",
      "2021-04-16     4/16/2021      0.0  Atlanta     6.0  Georgia         KATL   \n",
      "2021-04-17     4/17/2021      0.0  Atlanta     6.0  Georgia         KATL   \n",
      "2021-04-18     4/18/2021      0.0  Atlanta     6.0  Georgia         KATL   \n",
      "2021-04-19     4/19/2021      0.0  Atlanta     6.0  Georgia         KATL   \n",
      "2021-04-20     4/20/2021      0.0  Atlanta     6.0  Georgia         KATL   \n",
      "\n",
      "            temp_max_c  temp_mean_c  temp_min_c  \n",
      "2015-01-01        13.9     6.758333         0.0  \n",
      "2015-01-02         9.4     8.116667         6.7  \n",
      "2015-01-03        16.1    11.154167         8.9  \n",
      "2015-01-04        17.8    14.279167         6.7  \n",
      "2015-01-05         8.9     5.045833         1.1  \n",
      "...                ...          ...         ...  \n",
      "2021-04-16        19.4    14.695833        10.0  \n",
      "2021-04-17        21.7    16.795833        12.8  \n",
      "2021-04-18        22.2    16.720833        11.7  \n",
      "2021-04-19        21.7    16.154167        11.1  \n",
      "2021-04-20        23.3    17.491667        12.2  \n",
      "\n",
      "[2302 rows x 9 columns]\n",
      "<ipython-input-1230-fbd886507b9e>:3: DeprecationWarning: The default dtype for empty Series will be 'object' instead of 'float64' in a future version. Specify a dtype explicitly to silence this warning.\n",
      "  sTime = pd.Series(index=time)\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "time = pd.date_range(start = '2015-01-01', end = '2021-04-20', freq='D' )\n",
    "sTime = pd.Series(index=time)\n",
    "KATL = pd.concat([KATL, sTime[~sTime.index.isin(KATL.index)]]).sort_index()\n",
    "KATL = KATL.drop([0],axis=1)\n",
    "\n",
    "print(KATL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1231,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for /: 'str' and 'int'",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    141\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 142\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpressions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    143\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[0;32m    234\u001b[0m             \u001b[1;31m# error: \"None\" not callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 235\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0m_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# type: ignore[misc]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    236\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py\u001b[0m in \u001b[0;36m_evaluate_numexpr\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 120\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_evaluate_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop_str\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    121\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\computation\\expressions.py\u001b[0m in \u001b[0;36m_evaluate_standard\u001b[1;34m(op, op_str, a, b)\u001b[0m\n\u001b[0;32m     68\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'int'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1231-2b057bd7d969>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mKATL\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mKATL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mffill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mKATL\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbfill\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\common.py\u001b[0m in \u001b[0;36mnew_method\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitem_from_zerodim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mnew_method\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\arraylike.py\u001b[0m in \u001b[0;36m__truediv__\u001b[1;34m(self, other)\u001b[0m\n\u001b[0;32m    111\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__truediv__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__truediv__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 113\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_arith_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moperator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtruediv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    115\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0munpack_zerodim_and_defer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"__rtruediv__\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_arith_method\u001b[1;34m(self, other, op)\u001b[0m\n\u001b[0;32m   5980\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0malign_method_FRAME\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5981\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5982\u001b[1;33m         \u001b[0mnew_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch_frame_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mother\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5983\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_construct_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5984\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m_dispatch_frame_op\u001b[1;34m(self, right, func, axis)\u001b[0m\n\u001b[0;32m   6006\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6007\u001b[0m             \u001b[1;31m# i.e. scalar, faster than checking np.ndim(right) == 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6008\u001b[1;33m             \u001b[0mbm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marray_op\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mright\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   6009\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\managers.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, f, align_keys, ignore_failures, **kwargs)\u001b[0m\n\u001b[0;32m    423\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    424\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 425\u001b[1;33m                     \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    426\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    427\u001b[0m                     \u001b[0mapplied\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\internals\\blocks.py\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, **kwargs)\u001b[0m\n\u001b[0;32m    376\u001b[0m         \"\"\"\n\u001b[0;32m    377\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 378\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    379\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    380\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_split_op_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36marithmetic_op\u001b[1;34m(left, right, op)\u001b[0m\n\u001b[0;32m    187\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    188\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 189\u001b[1;33m             \u001b[0mres_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_na_arithmetic_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    191\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mres_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_na_arithmetic_op\u001b[1;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[0;32m    147\u001b[0m             \u001b[1;31m#  will handle complex numbers incorrectly, see GH#32047\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m             \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_masked_arith_op\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mright\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_cmp\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mis_scalar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mresult\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\ops\\array_ops.py\u001b[0m in \u001b[0;36m_masked_arith_op\u001b[1;34m(x, y, op)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0many\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merrstate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m                 \u001b[0mresult\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxrav\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m     \u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmaybe_upcast_putmask\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m~\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: unsupported operand type(s) for /: 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "KATL = (KATL.ffill()+KATL.bfill())/2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1232,
   "metadata": {},
   "outputs": [],
   "source": [
    "KATLinterpol = KATL['temp_mean_c'].interpolate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python388jvsc74a57bd0e96f6acdfbe39255c7875dee1b1acc7feafbd646fa220a924b021b041aac88e9",
   "display_name": "Python 3.8.8 64-bit ('base': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}